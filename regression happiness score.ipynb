{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":82143,"databundleVersionId":8951650,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Preliminary Round FIT Competition 2024**\n---\n### **Bismillah Dulu**\nUniversity of Brawijaya\n- Amira Ghina Nurfansepta\n- Shania Edina","metadata":{}},{"cell_type":"markdown","source":"## **Data Understanding**\nData understanding involves comprehensively exploring and analyzing a dataset to gain insights into its structure, characteristics, and potential issues. This process helps identify patterns, relationships, and anomalies, which are crucial for informed decision-making in data-driven projects.","metadata":{}},{"cell_type":"markdown","source":"### **Features**\n\n- **id** - City or Regency identifier\n- **city_or_regency** - Name of City or Regency\n- **year** - The year in which the data is recorded\n- **total_area** - Area of City or Regency (KM^2)\n- **population** - The Number of Residents in One City or Regency\n- **densities** - Density Level (Population/KM^2)\n- **traffic_density** - Categories for Traffic Density (Low/Medium/High)\n- **green_open_space** - Area of Green Open Space (KM^2)\n- **hdi** - Index of Human Development for Each City or Regency\n- **gross_regional_domestic_product** - Total Gross Value Added at Current Prices (Billion Rupiah)\n- **total_landfills** - Number of Landfills per City or Regency\n- **solid_waste_generated** - The amount of waste each City or Regency generated from various sources for a year (Tens of Tons)\n- **happiness_score** - Score to Measure The Level of Happiness for each city or Regency (0 - 100)","metadata":{}},{"cell_type":"markdown","source":"### **Import Library**\nInstall and import the required libraries.","metadata":{}},{"cell_type":"code","source":"pip install catboost","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:01:51.032558Z","iopub.execute_input":"2024-07-05T13:01:51.033045Z","iopub.status.idle":"2024-07-05T13:02:28.388454Z","shell.execute_reply.started":"2024-07-05T13:01:51.033011Z","shell.execute_reply":"2024-07-05T13:02:28.387043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import LabelEncoder, PolynomialFeatures, MinMaxScaler, StandardScaler\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:28.390777Z","iopub.execute_input":"2024-07-05T13:02:28.391207Z","iopub.status.idle":"2024-07-05T13:02:30.431429Z","shell.execute_reply.started":"2024-07-05T13:02:28.391163Z","shell.execute_reply":"2024-07-05T13:02:30.430322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Import Dataset**\nImport dataset to view existing data.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/preliminary-round-fit-competition-2024/train.csv')\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.432832Z","iopub.execute_input":"2024-07-05T13:02:30.433360Z","iopub.status.idle":"2024-07-05T13:02:30.489547Z","shell.execute_reply.started":"2024-07-05T13:02:30.433328Z","shell.execute_reply":"2024-07-05T13:02:30.488473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/preliminary-round-fit-competition-2024/test.csv')\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.491909Z","iopub.execute_input":"2024-07-05T13:02:30.492280Z","iopub.status.idle":"2024-07-05T13:02:30.518312Z","shell.execute_reply.started":"2024-07-05T13:02:30.492251Z","shell.execute_reply":"2024-07-05T13:02:30.517217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Number of Rows and Columns**\nKnowing how many rows and columns of each data.","metadata":{}},{"cell_type":"code","source":"print(\"Number of rows =\", df_train.shape[0])\nprint(\"Number of columns =\", df_train.shape[1])","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.519903Z","iopub.execute_input":"2024-07-05T13:02:30.520299Z","iopub.status.idle":"2024-07-05T13:02:30.526091Z","shell.execute_reply.started":"2024-07-05T13:02:30.520267Z","shell.execute_reply":"2024-07-05T13:02:30.525012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of rows =\", df_test.shape[0])\nprint(\"Number of columns =\", df_test.shape[1])","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.527523Z","iopub.execute_input":"2024-07-05T13:02:30.527905Z","iopub.status.idle":"2024-07-05T13:02:30.538747Z","shell.execute_reply.started":"2024-07-05T13:02:30.527875Z","shell.execute_reply":"2024-07-05T13:02:30.537449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Dataset Information**\nKnowing the data type and missing values in each data.","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.540223Z","iopub.execute_input":"2024-07-05T13:02:30.540580Z","iopub.status.idle":"2024-07-05T13:02:30.572224Z","shell.execute_reply.started":"2024-07-05T13:02:30.540550Z","shell.execute_reply":"2024-07-05T13:02:30.571163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.573521Z","iopub.execute_input":"2024-07-05T13:02:30.573870Z","iopub.status.idle":"2024-07-05T13:02:30.587711Z","shell.execute_reply.started":"2024-07-05T13:02:30.573841Z","shell.execute_reply":"2024-07-05T13:02:30.586489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are data types that do not match and must be corrected. All data has missing values in some columns.","metadata":{}},{"cell_type":"markdown","source":"### **Descriptive Statistics**\nKnowing descriptive statistics of each data, either object or numeric data.","metadata":{}},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.589269Z","iopub.execute_input":"2024-07-05T13:02:30.589699Z","iopub.status.idle":"2024-07-05T13:02:30.625687Z","shell.execute_reply.started":"2024-07-05T13:02:30.589660Z","shell.execute_reply":"2024-07-05T13:02:30.624613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.630520Z","iopub.execute_input":"2024-07-05T13:02:30.630910Z","iopub.status.idle":"2024-07-05T13:02:30.662450Z","shell.execute_reply.started":"2024-07-05T13:02:30.630879Z","shell.execute_reply":"2024-07-05T13:02:30.661242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.664198Z","iopub.execute_input":"2024-07-05T13:02:30.664570Z","iopub.status.idle":"2024-07-05T13:02:30.699058Z","shell.execute_reply.started":"2024-07-05T13:02:30.664535Z","shell.execute_reply":"2024-07-05T13:02:30.697727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.700508Z","iopub.execute_input":"2024-07-05T13:02:30.700893Z","iopub.status.idle":"2024-07-05T13:02:30.725048Z","shell.execute_reply.started":"2024-07-05T13:02:30.700861Z","shell.execute_reply":"2024-07-05T13:02:30.724032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Duplicated Data**\nChecking for duplicates in each data.","metadata":{}},{"cell_type":"code","source":"print(\"Number of duplicate data in the dataset:\", df_train.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.726276Z","iopub.execute_input":"2024-07-05T13:02:30.726604Z","iopub.status.idle":"2024-07-05T13:02:30.737786Z","shell.execute_reply.started":"2024-07-05T13:02:30.726575Z","shell.execute_reply":"2024-07-05T13:02:30.736551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of duplicate data in the dataset:\", df_test.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.739368Z","iopub.execute_input":"2024-07-05T13:02:30.739730Z","iopub.status.idle":"2024-07-05T13:02:30.749939Z","shell.execute_reply.started":"2024-07-05T13:02:30.739700Z","shell.execute_reply":"2024-07-05T13:02:30.748696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no duplication in any data.","metadata":{}},{"cell_type":"markdown","source":"### **Missing Value**","metadata":{}},{"cell_type":"code","source":"df_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.751598Z","iopub.execute_input":"2024-07-05T13:02:30.751972Z","iopub.status.idle":"2024-07-05T13:02:30.767618Z","shell.execute_reply.started":"2024-07-05T13:02:30.751943Z","shell.execute_reply":"2024-07-05T13:02:30.766351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.769034Z","iopub.execute_input":"2024-07-05T13:02:30.769440Z","iopub.status.idle":"2024-07-05T13:02:30.782799Z","shell.execute_reply.started":"2024-07-05T13:02:30.769411Z","shell.execute_reply":"2024-07-05T13:02:30.781561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All data has missing values in the green_open_space, total_landfills, and solid_waste_generated columns. The missing values must be filled in.","metadata":{}},{"cell_type":"markdown","source":"## **Data Preprocessing**\nData preprocessing refers to the cleaning, transformation, and preparation of raw data before analysis. It involves tasks such as handling missing data, removing outliers, standardizing or normalizing data, and encoding categorical variables, all aimed at ensuring the data is suitable and reliable for machine learning models or analytical processes.","metadata":{}},{"cell_type":"markdown","source":"### **Cleaning Numeric Data**\nCleaning numerical data to make it suitable and easier to process.","metadata":{}},{"cell_type":"code","source":"def clean_numeric_data(column):\n    return pd.to_numeric(column.astype(str).str.replace(',', ''), errors='coerce')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['total_area (km2)'] = clean_numeric_data(df_train['total_area (km2)'])\ndf_train['population'] = clean_numeric_data(df_train['population'])\ndf_train['gross_regional_domestic_product'] = clean_numeric_data(df_train['gross_regional_domestic_product'])\ndf_train['solid_waste_generated'] = clean_numeric_data(df_train['solid_waste_generated'])\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.784265Z","iopub.execute_input":"2024-07-05T13:02:30.784722Z","iopub.status.idle":"2024-07-05T13:02:30.818536Z","shell.execute_reply.started":"2024-07-05T13:02:30.784679Z","shell.execute_reply":"2024-07-05T13:02:30.817461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['total_area (km2)'] = clean_numeric_data(df_test['total_area (km2)'])\ndf_test['population'] = clean_numeric_data(df_test['population'])\ndf_test['gross_regional_domestic_product'] = clean_numeric_data(df_test['gross_regional_domestic_product'])\ndf_test['solid_waste_generated'] = clean_numeric_data(df_test['solid_waste_generated'])\n\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.820316Z","iopub.execute_input":"2024-07-05T13:02:30.820704Z","iopub.status.idle":"2024-07-05T13:02:30.849393Z","shell.execute_reply.started":"2024-07-05T13:02:30.820674Z","shell.execute_reply":"2024-07-05T13:02:30.848230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Converting Data Types**\nChange the data type to match what it should be.","metadata":{}},{"cell_type":"code","source":"df_train['total_area (km2)'] = pd.to_numeric(df_train['total_area (km2)'], errors='coerce')\ndf_train['population'] = pd.to_numeric(df_train['population'], errors='coerce').astype('Int64')\ndf_train['green_open_space'] = pd.to_numeric(df_train['green_open_space'], errors='coerce')\ndf_train['gross_regional_domestic_product'] = pd.to_numeric(df_train['gross_regional_domestic_product'], errors='coerce').astype('Int64')\ndf_train['solid_waste_generated'] = pd.to_numeric(df_train['solid_waste_generated'], errors='coerce')\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.850948Z","iopub.execute_input":"2024-07-05T13:02:30.851407Z","iopub.status.idle":"2024-07-05T13:02:30.874645Z","shell.execute_reply.started":"2024-07-05T13:02:30.851365Z","shell.execute_reply":"2024-07-05T13:02:30.873504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['total_area (km2)'] = pd.to_numeric(df_test['total_area (km2)'], errors='coerce')\ndf_test['population'] = pd.to_numeric(df_test['population'], errors='coerce').astype('Int64')\ndf_test['green_open_space'] = pd.to_numeric(df_test['green_open_space'], errors='coerce')\ndf_test['gross_regional_domestic_product'] = pd.to_numeric(df_test['gross_regional_domestic_product'], errors='coerce').astype('Int64')\ndf_test['solid_waste_generated'] = pd.to_numeric(df_test['solid_waste_generated'], errors='coerce')\ndf_test.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.876169Z","iopub.execute_input":"2024-07-05T13:02:30.876629Z","iopub.status.idle":"2024-07-05T13:02:30.894989Z","shell.execute_reply.started":"2024-07-05T13:02:30.876590Z","shell.execute_reply":"2024-07-05T13:02:30.893569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Drop Missing Value**\nRemoving missing values in some columns.","metadata":{}},{"cell_type":"code","source":"# df_train = df_train.dropna(subset=['green_open_space', 'total_landfills', 'solid_waste_generated'])\n# df_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.896809Z","iopub.execute_input":"2024-07-05T13:02:30.897843Z","iopub.status.idle":"2024-07-05T13:02:30.907349Z","shell.execute_reply.started":"2024-07-05T13:02:30.897780Z","shell.execute_reply":"2024-07-05T13:02:30.906279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test = df_test.dropna(subset=['green_open_space', 'total_landfills', 'solid_waste_generated'])\n# df_test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.908895Z","iopub.execute_input":"2024-07-05T13:02:30.909282Z","iopub.status.idle":"2024-07-05T13:02:30.919020Z","shell.execute_reply.started":"2024-07-05T13:02:30.909241Z","shell.execute_reply":"2024-07-05T13:02:30.917744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the analysis of the results obtained, the MSE value by removing missing values is worse than imputation.","metadata":{}},{"cell_type":"markdown","source":"### **Input Missing Value**\nPerform imputation with KNN Imputer for numerical data.","metadata":{}},{"cell_type":"code","source":"imputer = KNNImputer(n_neighbors=5)\ncolumns_to_impute = ['green_open_space', 'total_landfills', 'solid_waste_generated']\ndf_train[columns_to_impute] = imputer.fit_transform(df_train[columns_to_impute])\ndf_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:30.920645Z","iopub.execute_input":"2024-07-05T13:02:30.921274Z","iopub.status.idle":"2024-07-05T13:02:31.014375Z","shell.execute_reply.started":"2024-07-05T13:02:30.921228Z","shell.execute_reply":"2024-07-05T13:02:31.013199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputer = KNNImputer(n_neighbors=5)\ncolumns_to_impute = ['green_open_space', 'total_landfills', 'solid_waste_generated']\ndf_test[columns_to_impute] = imputer.fit_transform(df_test[columns_to_impute])\ndf_test.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:31.015916Z","iopub.execute_input":"2024-07-05T13:02:31.016912Z","iopub.status.idle":"2024-07-05T13:02:31.054408Z","shell.execute_reply.started":"2024-07-05T13:02:31.016869Z","shell.execute_reply":"2024-07-05T13:02:31.053202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.impute import SimpleImputer\n\n# # Inisialisasi SimpleImputer dengan strategi 'mean'\n# mean_imputer = SimpleImputer(strategy='mean')\n\n# # Kolom yang akan diimputasi\n# columns_to_impute = ['green_open_space', 'total_landfills', 'solid_waste_generated']\n\n# # Imputasi nilai yang hilang\n# df[columns_to_impute] = mean_imputer.fit_transform(df[columns_to_impute])\n\n# # Tampilkan hasil setelah diimputasi\n# df.info()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.impute import SimpleImputer\n\n# # Inisialisasi SimpleImputer dengan strategi 'median'\n# median_imputer = SimpleImputer(strategy='median')\n\n# # Kolom yang akan diimputasi\n# columns_to_impute = ['green_open_space', 'total_landfills', 'solid_waste_generated']\n\n# # Imputasi nilai yang hilang\n# df[columns_to_impute] = median_imputer.fit_transform(df[columns_to_impute])\n\n# # Tampilkan hasil setelah diimputasi\n# df.info()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.impute import SimpleImputer\n\n# # Inisialisasi SimpleImputer dengan strategi 'most_frequent' untuk data kategorikal\n# mode_imputer = SimpleImputer(strategy='most_frequent')\n\n# # Kolom yang akan diimputasi\n# columns_to_impute = ['green_open_space', 'total_landfills', 'solid_waste_generated']\n\n# # Imputasi nilai yang hilang\n# df[columns_to_impute] = mode_imputer.fit_transform(df[columns_to_impute])\n\n# # Tampilkan hasil setelah diimputasi\n# df.info()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.experimental import enable_iterative_imputer\n# from sklearn.impute import IterativeImputer\n\n# # Inisialisasi IterativeImputer\n# iterative_imputer = IterativeImputer()\n\n# # Imputasi nilai yang hilang\n# df[columns_to_impute] = iterative_imputer.fit_transform(df[columns_to_impute])\n\n# # Tampilkan hasil setelah diimputasi\n# df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After experimenting several times, the best n_neighbors value is 5. ","metadata":{}},{"cell_type":"markdown","source":"### **Feature Engineering**\nFeature engineering involves creating new features or transforming existing ones from raw data that can enhance the performance of machine learning models. This process aims to extract meaningful information, reduce noise, or improve the representation of data, ultimately improving the model's ability to make accurate predictions or classifications. Examples include creating interaction terms, scaling features, or encoding temporal information.","metadata":{}},{"cell_type":"code","source":"def split_id(id):\n    str_id = str(id)\n    split1 = str_id[:1]\n    split2 = str_id[1:2]\n    split3 = str_id[:2]\n    split4 = str_id[3:4]\n    return pd.Series([split1, split2,split3,split4])","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:31.056237Z","iopub.execute_input":"2024-07-05T13:02:31.056932Z","iopub.status.idle":"2024-07-05T13:02:31.064716Z","shell.execute_reply.started":"2024-07-05T13:02:31.056893Z","shell.execute_reply":"2024-07-05T13:02:31.062741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[['split1', 'split2','split3','split4']] = df_train['id'].apply(split_id)\ndf_train[['id', 'split1', 'split2','split3','split4']]\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:31.067673Z","iopub.execute_input":"2024-07-05T13:02:31.068581Z","iopub.status.idle":"2024-07-05T13:02:31.214927Z","shell.execute_reply.started":"2024-07-05T13:02:31.068538Z","shell.execute_reply":"2024-07-05T13:02:31.213724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[['split1', 'split2','split3','split4']] = df_test['id'].apply(split_id)\ndf_test[['id', 'split1', 'split2','split3','split4']]\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:31.216626Z","iopub.execute_input":"2024-07-05T13:02:31.217213Z","iopub.status.idle":"2024-07-05T13:02:31.278464Z","shell.execute_reply.started":"2024-07-05T13:02:31.217173Z","shell.execute_reply":"2024-07-05T13:02:31.277348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We analyzed the id, and found that there is a code for each digit in the id.\n- split1 is an island\n- split2 is the province increment for each island\n- split3 is the province\n- split4 is the grouping of cities and districts","metadata":{}},{"cell_type":"code","source":"poly = PolynomialFeatures(degree=2, include_bias=False)\nhdi_poly = poly.fit_transform(df_train[['hdi']])\ndf_train['hdi_squared'] = hdi_poly[:, 1]\n\nscaler = StandardScaler()\ndf_train['standardized_hdi'] = scaler.fit_transform(df_train[['hdi']])\n\ndf_train['hdi_binned'] = pd.qcut(df_train['hdi'], q=4, labels=['Low', 'Medium-Low', 'Medium-High', 'High'])\ndf_train['hdi_x_densities'] = df_train['hdi'] * df_train['densities']\ndf_train['hdi_x_gross'] = df_train['hdi'] * df_train['gross_regional_domestic_product']\ndf_train['hdi_log'] = np.log(df_train['hdi'] + 1)\n\ndf_train['densities_binned'] = pd.qcut(df_train['densities'], q=3, labels=['Low', 'Medium', 'High'])\ndf_train['density_green_space'] = df_train['green_open_space'] / df_train['total_area (km2)']\n\ndf_train['gdp_per_capita'] = df_train['gross_regional_domestic_product'] / df_train['population']\ndf_train['log_grdp'] = np.log(df_train['gross_regional_domestic_product'] + 1)\ndf_train['lagged_gdp_growth'] = df_train['gross_regional_domestic_product'].diff()\n\ndf_train['green_space_per_capita'] = df_train['green_open_space'] / df_train['population']\ndf_train['waste_per_capita'] = df_train['solid_waste_generated'] / df_train['population']\ndf_train['sqrt_landfills'] = np.sqrt(df_train['total_landfills'])\ndf_train['population_log'] = np.log(df_train['population'] + 1)\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:31.284981Z","iopub.execute_input":"2024-07-05T13:02:31.285398Z","iopub.status.idle":"2024-07-05T13:02:31.351406Z","shell.execute_reply.started":"2024-07-05T13:02:31.285364Z","shell.execute_reply":"2024-07-05T13:02:31.350222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We did some feature engineering for the other columns to add more data and knowledge to the model.","metadata":{}},{"cell_type":"markdown","source":"### **Label Encoding**\nLabel encoding is a technique used in feature engineering where categorical data is converted into numerical form. Each unique category is assigned a unique integer, typically starting from 0 or 1 up to the number of distinct categories minus one. It's useful for algorithms that require numerical inputs, but it may not be suitable for categorical variables with no inherent order, as it could introduce unintended relationships.","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\n\nobject_columns = ['city_or_regency', 'traffic_density', 'hdi_binned', 'densities_binned', 'split1', 'split3']\nfor col in object_columns:\n    df_train[col] = label_encoder.fit_transform(df_train[col])\n\ndf_train.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:31.352999Z","iopub.execute_input":"2024-07-05T13:02:31.353398Z","iopub.status.idle":"2024-07-05T13:02:31.455436Z","shell.execute_reply.started":"2024-07-05T13:02:31.353364Z","shell.execute_reply":"2024-07-05T13:02:31.454304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\n\nobject_columns = ['city_or_regency', 'traffic_density', 'split1', 'split3']\nfor col in object_columns:\n    df_test[col] = label_encoder.fit_transform(df_test[col])\n\ndf_test.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:31.457031Z","iopub.execute_input":"2024-07-05T13:02:31.457416Z","iopub.status.idle":"2024-07-05T13:02:31.514368Z","shell.execute_reply.started":"2024-07-05T13:02:31.457383Z","shell.execute_reply":"2024-07-05T13:02:31.513096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Normalization**\nNormalization is a preprocessing technique used to rescale numeric data to a common scale, typically between 0 and 1. It ensures that all features contribute equally to the analysis and prevents features with larger numeric ranges from dominating those with smaller ranges. Common normalization techniques include Min-Max scaling and Z-score standardization.","metadata":{}},{"cell_type":"code","source":"# scaler = MinMaxScaler()\n\n# df['total_area (km2)'] = scaler.fit_transform(df[['total_area (km2)']])\n# df['population'] = scaler.fit_transform(df[['population']])\n# df['densities'] = scaler.fit_transform(df[['densities']])\n# df['green_open_space'] = scaler.fit_transform(df[['green_open_space']])\n# df['hdi'] = scaler.fit_transform(df[['hdi']])\n# df['gross_regional_domestic_product'] = scaler.fit_transform(df[['gross_regional_domestic_product']])\n# df['total_landfills'] = scaler.fit_transform(df[['total_landfills']])\n# df['solid_waste_generated'] = scaler.fit_transform(df[['solid_waste_generated']])\n\n# df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:31.515969Z","iopub.execute_input":"2024-07-05T13:02:31.516434Z","iopub.status.idle":"2024-07-05T13:02:31.522111Z","shell.execute_reply.started":"2024-07-05T13:02:31.516377Z","shell.execute_reply":"2024-07-05T13:02:31.521099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We tried normalizing the data, and the MSE result was the same as the non-normalized one. Therefore, normalization does not need to be applied.","metadata":{}},{"cell_type":"markdown","source":"### **Feature Selection**\nFeature selection is the process of choosing a subset of relevant features (variables, predictors) from a larger set of available features to use in model construction. It aims to improve model performance by reducing overfitting, simplifying interpretation, and decreasing computational cost. Techniques include statistical tests, feature importance from models, and algorithms like Recursive Feature Elimination (RFE).","metadata":{}},{"cell_type":"code","source":"X = df_train.drop('happiness_score', axis=1)\ny = df_train['happiness_score']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:12:34.956474Z","iopub.execute_input":"2024-07-05T13:12:34.956923Z","iopub.status.idle":"2024-07-05T13:12:34.975199Z","shell.execute_reply.started":"2024-07-05T13:12:34.956890Z","shell.execute_reply":"2024-07-05T13:12:34.973966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CatBoostRegressor(verbose=0)\nmodel.fit(X_train, y_train)\n\nfeature_importance = model.get_feature_importance()\n\nplt.figure(figsize=(10, 6))\nplt.barh(X_train.columns, feature_importance)\nplt.xlabel('Feature Importance')\nplt.ylabel('Features')\nplt.title('Feature Importance Plot')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:12:35.176043Z","iopub.execute_input":"2024-07-05T13:12:35.177107Z","iopub.status.idle":"2024-07-05T13:12:39.093044Z","shell.execute_reply.started":"2024-07-05T13:12:35.177066Z","shell.execute_reply":"2024-07-05T13:12:39.091819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We perform feature selection with the feature importance of the CatBoost model. The MSE results obtained are still not good.","metadata":{}},{"cell_type":"code","source":"correlation_matrix = df_train.corr()\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='Blues')\nplt.title('Correlation Matrix')\nplt.show()\n\ncorrelation_with_target = correlation_matrix[\"happiness_score\"].drop(\"happiness_score\")\nthreshold = 0.1\nselected_features = correlation_with_target[abs(correlation_with_target) > threshold].index.tolist()\n\nprint(correlation_with_target)\nprint(f\"Selected features: {selected_features}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:31.550105Z","iopub.execute_input":"2024-07-05T13:02:31.550583Z","iopub.status.idle":"2024-07-05T13:02:34.807738Z","shell.execute_reply.started":"2024-07-05T13:02:31.550544Z","shell.execute_reply":"2024-07-05T13:02:34.806530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We try to do feature selection with the correlation of each feature. We tried one by one in bruteforce.","metadata":{}},{"cell_type":"markdown","source":"## **Exploratory Data Analysis**\nExploratory Data Analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. It helps uncover patterns, trends, relationships, and anomalies in the data, providing insights that inform further analysis or model building. EDA typically involves tasks such as data visualization, summary statistics, and correlation analysis to understand the nature of the data before applying more complex techniques.","metadata":{}},{"cell_type":"markdown","source":"### **Feature and Label Distribution**\nUnderstanding the distribution of features and labels helps identify patterns, trends, and potential anomalies within the data, facilitating better model training and performance.","metadata":{}},{"cell_type":"code","source":"features = ['year', 'split1', 'split3']\nfor column in features:\n    plt.figure(figsize=(10, 5))\n    sns.histplot(df_train[column], kde=True, bins=30)\n    plt.title(f'Distribution of {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    plt.show()\n\nplt.figure(figsize=(10, 5))\nsns.histplot(df_train['happiness_score'], kde=True, bins=30)\nplt.title('Distribution of Happiness Score')\nplt.xlabel('Happiness Score')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:23:57.696982Z","iopub.execute_input":"2024-07-05T13:23:57.697904Z","iopub.status.idle":"2024-07-05T13:23:59.641344Z","shell.execute_reply.started":"2024-07-05T13:23:57.697862Z","shell.execute_reply":"2024-07-05T13:23:59.640047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_features = df_train.columns.tolist()\nnum_cols = 3\nnum_rows = (len(all_features) + num_cols - 1) // num_cols\n\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\naxes = axes.flatten()\n\nfor i, column in enumerate(all_features):\n    sns.histplot(df_train[column], kde=True, bins=30, ax=axes[i])\n    axes[i].set_title(f'Distribution of {column}')\n    axes[i].set_xlabel(column)\n    axes[i].set_ylabel('Frequency')\n\nfor i in range(len(all_features), len(axes)):\n    fig.delaxes(axes[i])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:40:14.246348Z","iopub.execute_input":"2024-07-05T13:40:14.246936Z","iopub.status.idle":"2024-07-05T13:40:27.742094Z","shell.execute_reply.started":"2024-07-05T13:40:14.246892Z","shell.execute_reply":"2024-07-05T13:40:27.740788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the visualization, we can observe the data distribution for each feature; some features have a normal distribution, while others are skewed.\n- The `year` feature shows an equal distribution of data for 2022 and 2023.\n- The `split1` and `split3` features exhibit varied distributions.\n- The `happiness_score` is almost normally distributed.","metadata":{}},{"cell_type":"markdown","source":"### **Descriptive Statistics**\nDescriptive statistics provide summary insights about the central tendency, dispersion, and shape of the data's distribution, enabling quick comprehension of the data's characteristics.","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ndf_train.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:34:07.580852Z","iopub.execute_input":"2024-07-05T13:34:07.581692Z","iopub.status.idle":"2024-07-05T13:34:07.714468Z","shell.execute_reply.started":"2024-07-05T13:34:07.581633Z","shell.execute_reply":"2024-07-05T13:34:07.713207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Feature Correlation**\nAnalyzing feature correlations helps identify relationships between variables, which can inform feature selection and improve the model's predictive power.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\ncorrelation_matrix = df_train[features + ['happiness_score']].corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:25:50.295366Z","iopub.execute_input":"2024-07-05T13:25:50.296396Z","iopub.status.idle":"2024-07-05T13:25:50.724393Z","shell.execute_reply.started":"2024-07-05T13:25:50.296340Z","shell.execute_reply":"2024-07-05T13:25:50.723251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation matrix indicates that all three features have a positive correlation with the happiness score. These correlations suggest that each of these features is meaningfully related to the happiness score.","metadata":{}},{"cell_type":"markdown","source":"### **Outlier Identification**\nDetecting outliers is crucial for ensuring data quality, as outliers can significantly skew model training and lead to poor generalization.","metadata":{}},{"cell_type":"code","source":"for column in features:\n    plt.figure(figsize=(10, 5))\n    sns.boxplot(x=df_train[column])\n    plt.title(f'Boxplot of {column}')\n    plt.show()\n\nplt.figure(figsize=(10, 5))\nsns.boxplot(x=df_train['happiness_score'])\nplt.title('Boxplot of Happiness Score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:26:19.988976Z","iopub.execute_input":"2024-07-05T13:26:19.990369Z","iopub.status.idle":"2024-07-05T13:26:20.786182Z","shell.execute_reply.started":"2024-07-05T13:26:19.990318Z","shell.execute_reply":"2024-07-05T13:26:20.784725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the boxplot visualization, there are no outliers in the features `year`, `split1`, and `split3`. However, the `happiness_score` does have a few outliers. ","metadata":{}},{"cell_type":"code","source":"\nnumeric_features = df_train.select_dtypes(include=[np.number]).columns.tolist()\nnum_cols = 3\nnum_rows = (len(numeric_features) + num_cols - 1) // num_cols\n\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\naxes = axes.flatten()\n\nfor i, column in enumerate(numeric_features):\n    sns.boxplot(x=df_train[column], ax=axes[i])\n    axes[i].set_title(f'Boxplot of {column}')\n    axes[i].set_xlabel(column)\n\nfor i in range(len(numeric_features), len(axes)):\n    fig.delaxes(axes[i])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T14:01:53.017015Z","iopub.execute_input":"2024-07-05T14:01:53.017800Z","iopub.status.idle":"2024-07-05T14:01:58.452001Z","shell.execute_reply.started":"2024-07-05T14:01:53.017762Z","shell.execute_reply":"2024-07-05T14:01:58.450495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the boxplot visualizations, several features exhibit a significant number of outliers","metadata":{}},{"cell_type":"markdown","source":"### **Multivariate Visualization**\nMultivariate visualizations provide a comprehensive view of the relationships between multiple features simultaneously, aiding in the identification of complex patterns and interactions.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df_train[features + ['happiness_score']])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:28:28.404090Z","iopub.execute_input":"2024-07-05T13:28:28.405498Z","iopub.status.idle":"2024-07-05T13:28:33.371687Z","shell.execute_reply.started":"2024-07-05T13:28:28.405427Z","shell.execute_reply":"2024-07-05T13:28:33.370406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Year: Data is evenly distributed between 2022 and 2023.\n* Split1 and Split3: Both show a varied distribution across different values.\n* Happiness Score: Nearly forms a normal distribution, with most scores clustered around the mean.\n\nRelationships:\n* `Split1` and `Split3` have a clear, positive relationship.\n* `Year` has a distinct separation, especially noticeable in `split1` and `split3`.\n* `Happiness Score` shows spread relationships with other features.","metadata":{}},{"cell_type":"code","source":"# # scatter plot\n# plt.figure(figsize=(10, 6))\n# sns.scatterplot(x=df['green_open_space'], y=df['solid_waste_generated'])\n# plt.title('Scatter Plot of Green Open Space vs Solid Waste Generated')\n# plt.xlabel('Green Open Space')\n# plt.ylabel('Solid Waste Generated')\n# plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Plot line plot\n# plt.figure(figsize=(10, 6))\n# sns.lineplot(data=df, x='year', y='solid_waste_generated')\n# plt.title('Line Plot of Solid Waste Generated Over Years')\n# plt.xlabel('Year')\n# plt.ylabel('Solid Waste Generated')\n# plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Plot bar plot\n# plt.figure(figsize=(10, 6))\n# sns.barplot(x=df['city'], y=df['solid_waste_generated'])\n# plt.title('Bar Plot of Solid Waste Generated by City')\n# plt.xlabel('City')\n# plt.ylabel('Solid Waste Generated')\n# plt.xticks(rotation=90)\n# plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = df['city'].value_counts()\n\n# # Plot pie chart\n# plt.figure(figsize=(10, 6))\n# plt.pie(data, labels=data.index, autopct='%1.1f%%', startangle=140)\n# plt.title('Pie Chart of City Distribution')\n# plt.axis('equal')\n# plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Modeling (Regression)**\nModeling in the context of regression involves using statistical techniques to build a predictive model that estimates the relationship between one or more independent variables (predictors) and a dependent variable (target). The goal is to create a function that best fits the data, allowing predictions of the target variable for new data points. Techniques range from simple linear regression to more complex methods like polynomial regression, ridge regression, or machine learning algorithms such as random forests or gradient boosting. Evaluation of regression models typically involves metrics like mean squared error (MSE) or R-squared to assess predictive accuracy.","metadata":{}},{"cell_type":"markdown","source":"### **Split Data**\nSplitting data refers to dividing a dataset into two or more subsets for different purposes, typically for training and evaluating machine learning models.","metadata":{}},{"cell_type":"code","source":"X = df_train[['year', 'split3', 'split1']]\ny = df_train['happiness_score']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:34.834425Z","iopub.execute_input":"2024-07-05T13:02:34.834885Z","iopub.status.idle":"2024-07-05T13:02:34.851291Z","shell.execute_reply.started":"2024-07-05T13:02:34.834843Z","shell.execute_reply":"2024-07-05T13:02:34.849851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing = df_test[['year', 'split3', 'split1']]","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:34.852858Z","iopub.execute_input":"2024-07-05T13:02:34.853279Z","iopub.status.idle":"2024-07-05T13:02:34.863673Z","shell.execute_reply.started":"2024-07-05T13:02:34.853246Z","shell.execute_reply":"2024-07-05T13:02:34.862474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After trying repeatedly, with different features in bruteforce, the 3 best features that affect the model and MSE value are year, split3, and split1.","metadata":{}},{"cell_type":"markdown","source":"### **Resampling**\nResampling refers to techniques used to repeatedly draw samples from a dataset to improve statistical inference and model performance. It's particularly useful in scenarios where data is limited or imbalanced.","metadata":{}},{"cell_type":"code","source":"# def add_gaussian_noise(X, y, mean=0, std=0.1, n_samples=10):\n#     X_augmented = []\n#     y_augmented = []\n#     for _ in range(n_samples):\n#         noise = np.random.normal(mean, std, X.shape)\n#         X_augmented.append(X + noise)\n#         y_augmented.append(y)\n#     return np.vstack(X_augmented), np.hstack(y_augmented)\n\n# X_train, y_train = add_gaussian_noise(X_train, y_train)\n\n# print(y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:34.865591Z","iopub.execute_input":"2024-07-05T13:02:34.866547Z","iopub.status.idle":"2024-07-05T13:02:34.875089Z","shell.execute_reply.started":"2024-07-05T13:02:34.866506Z","shell.execute_reply":"2024-07-05T13:02:34.873949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After resampling, the MSE results obtained are still not good enough so there is no need for resampling.","metadata":{}},{"cell_type":"markdown","source":"### **Tuning Hyperparameter**\nTuning hyperparameters involves the process of selecting the optimal values for parameters that are not directly learned during model training but rather set before training begins.","metadata":{}},{"cell_type":"code","source":"# model = CatBoostRegressor(verbose=0)\n\n# param_grid = {\n#     'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.1],\n#     'iterations': [500,700, 1000],\n# }\n\n# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n# grid_search.fit(X_train, y_train)\n\n# best_params = grid_search.best_params_\n# print(\"Best parameters found: \", best_params)\n\n# best_model = grid_search.best_estimator_\n# best_model.fit(X_train, y_train)\n\n# y_pred = best_model.predict(X_test)\n# mse = mean_squared_error(y_test, y_pred)\n# print(\"Mean Squared Error on test data: \", mse)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:34.876443Z","iopub.execute_input":"2024-07-05T13:02:34.876857Z","iopub.status.idle":"2024-07-05T13:02:34.890897Z","shell.execute_reply.started":"2024-07-05T13:02:34.876826Z","shell.execute_reply":"2024-07-05T13:02:34.889665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Modeling With CatBoost**\nCatBoost Regressor is a high-performance machine learning algorithm designed specifically for regression tasks, particularly suited for tabular data. It distinguishes itself by its ability to handle categorical variables automatically, without preprocessing, which simplifies data preparation and often improves model accuracy. CatBoost incorporates built-in regularization techniques to mitigate overfitting and utilizes gradient-based learning with ordered boosting to optimize training efficiency. It supports GPU acceleration for faster training on large datasets and has demonstrated competitive performance compared to other popular boosting algorithms like XGBoost and LightGBM. Overall, CatBoost Regressor is a robust choice for regression problems where both predictive accuracy and computational efficiency are priorities.","metadata":{}},{"cell_type":"code","source":"model = CatBoostRegressor(iterations=1000,\n                          learning_rate=0.02,  \n                          loss_function='RMSE',  \n                          random_state=0,\n                          verbose=0)\n\ncv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', verbose=100)\n\ncv_mse_scores = -cv_scores\n\nprint(\"Cross-Validation MSE scores for each fold:\", cv_mse_scores)\nprint(\"Average MSE score:\", cv_mse_scores.mean())","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:34.892269Z","iopub.execute_input":"2024-07-05T13:02:34.892670Z","iopub.status.idle":"2024-07-05T13:02:37.192734Z","shell.execute_reply.started":"2024-07-05T13:02:34.892637Z","shell.execute_reply":"2024-07-05T13:02:37.191460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We perform cross validation to determine whether the prediction results are overfitting or not.","metadata":{}},{"cell_type":"code","source":"model.fit(X_train, y_train, verbose=100)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:37.194245Z","iopub.execute_input":"2024-07-05T13:02:37.194699Z","iopub.status.idle":"2024-07-05T13:02:37.663186Z","shell.execute_reply.started":"2024-07-05T13:02:37.194659Z","shell.execute_reply":"2024-07-05T13:02:37.661919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import GradientBoostingRegressor\n\n# model_gbm = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.02, random_state=0)\n# model_gbm.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from lightgbm import LGBMRegressor\n\n# model_lgbm = LGBMRegressor(n_estimators=1000, learning_rate=0.02, random_state=0)\n# model_lgbm.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import AdaBoostRegressor\n\n# model_adaboost = AdaBoostRegressor(n_estimators=1000, learning_rate=0.02, random_state=0)\n# model_adaboost.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from xgboost import XGBRegressor\n\n# model_xgboost = XGBRegressor(n_estimators=1000, learning_rate=0.02, random_state=0)\n# model_xgboost.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from ngboost import NGBRegressor\n\n# model_ngboost = NGBRegressor(n_estimators=1000, learning_rate=0.02, random_state=0)\n# model_ngboost.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.experimental import enable_hist_gradient_boosting\n# from sklearn.ensemble import HistGradientBoostingRegressor\n\n# model_hist_gbm = HistGradientBoostingRegressor(max_iter=1000, learning_rate=0.02, random_state=0)\n# model_hist_gbm.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestRegressor\n\n# model_rf = RandomForestRegressor(n_estimators=1000, random_state=0)\n# model_rf.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.tree import DecisionTreeRegressor\n\n# model_dt = DecisionTreeRegressor(random_state=0)\n# model_dt.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.neighbors import KNeighborsRegressor\n\n# model_knn = KNeighborsRegressor(n_neighbors=5)\n# model_knn.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.naive_bayes import GaussianNB\n\n# model_nb = GaussianNB()\n# model_nb.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.svm import SVR\n\n# model_svm = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n# model_svm.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.linear_model import LogisticRegression\n\n# model_log_reg = LogisticRegression(random_state=0)\n# model_log_reg.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import StackingRegressor\n# from sklearn.linear_model import LinearRegression\n\n# estimators = [\n#     ('rf', RandomForestRegressor(n_estimators=1000, random_state=0)),\n#     ('gbm', GradientBoostingRegressor(n_estimators=1000, learning_rate=0.02, random_state=0))\n# ]\n\n# model_stacking = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n# model_stacking.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import VotingRegressor\n\n# estimators = [\n#     ('rf', RandomForestRegressor(n_estimators=1000, random_state=0)),\n#     ('gbm', GradientBoostingRegressor(n_estimators=1000, learning_rate=0.02, random_state=0))\n# ]\n\n# model_voting = VotingRegressor(estimators=estimators)\n# model_voting.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have tried several other models as well as hyperparameter tuning. It was found that CatBoost is the best model with the above tuning. Some of the models we have tried are GBM, LGBM, AdaBoost, XGBoost, NGBoost, HistGBM, Random Forest, Decision Tree, KNN, Naive Bayes, SVM, Logistic Regression, Stacking Approach, and Voting Approach.","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(f'MSE: {mse:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:37.664471Z","iopub.execute_input":"2024-07-05T13:02:37.664831Z","iopub.status.idle":"2024-07-05T13:02:37.675468Z","shell.execute_reply.started":"2024-07-05T13:02:37.664800Z","shell.execute_reply":"2024-07-05T13:02:37.674218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MSE of 1.361 is the best value we managed to get.","metadata":{}},{"cell_type":"markdown","source":"## **Result**\nModel results are applied to test data for submission.","metadata":{}},{"cell_type":"code","source":"prediction = model.predict(testing)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:37.676983Z","iopub.execute_input":"2024-07-05T13:02:37.677465Z","iopub.status.idle":"2024-07-05T13:02:37.691367Z","shell.execute_reply.started":"2024-07-05T13:02:37.677424Z","shell.execute_reply":"2024-07-05T13:02:37.689948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'id': df_test['id'], 'happiness_score' : prediction})\noutput.to_csv('submission.csv', index=False)\noutput","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:02:37.692969Z","iopub.execute_input":"2024-07-05T13:02:37.693460Z","iopub.status.idle":"2024-07-05T13:02:37.716950Z","shell.execute_reply.started":"2024-07-05T13:02:37.693427Z","shell.execute_reply":"2024-07-05T13:02:37.715810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Conclusion**\n- `year` has a moderate positive correlation with `happiness_score` (0.35), indicating that the year might significantly impact the happiness score. `Split1` (0.18) and `Split3` (0.17) also have moderate positive correlations with `happiness_score`, suggesting they are meaningful features for prediction.\n- `year`, `split1`, and `split3` have no significant outliers. `happiness_score` feature has a few outliers\n- CatBoost is the model that obtained the best MSE value compared to other models, with a value of 1.361\n- The feature set currently used is not enough to adequately predict happiness_score. Additional or more relevant features are needed to improve the prediction accuracy of the model.","metadata":{}},{"cell_type":"markdown","source":"## **Suggestion**\n- Expand the dataset to cover a longer timeframe and incorporate features that are more directly linked to happiness scores, which could potentially improve the model's predictive capabilities.","metadata":{}}]}